{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8128762,"sourceType":"datasetVersion","datasetId":4804383}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Chains in Langchain**","metadata":{}},{"cell_type":"markdown","source":"**Utilizing 1hr course LangChain for LLM Application Development,, I'm adapting similar methodologies for Llama2 from Hugging Face. Given the limited resources available, this endeavor aims to provide valuable insights and practical applications for individuals who wants to use different styles than chat_models module in langchain**","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade langchain openai  -q\n!pip install -q accelerate -U\n!pip install transformers -q\n!pip install -q bitsandbytes==0.40.2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-15T18:13:39.401048Z","iopub.execute_input":"2024-04-15T18:13:39.402074Z","iopub.status.idle":"2024-04-15T18:14:35.057822Z","shell.execute_reply.started":"2024-04-15T18:13:39.402033Z","shell.execute_reply":"2024-04-15T18:14:35.056446Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **Loading Model from HuggingFace**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import cuda, bfloat16\nimport transformers\n\nmodel_id = 'meta-llama/Llama-2-13b-chat-hf'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)\n\n# begin initializing HF items, need auth token for these\nhf_auth = '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'\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n    use_auth_token=hf_auth\n)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map= {\"\": 0},\n    use_auth_token=hf_auth\n)\nmodel.eval()\nprint(f\"Model loaded on {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:14:35.060056Z","iopub.execute_input":"2024-04-15T18:14:35.060367Z","iopub.status.idle":"2024-04-15T18:19:09.200061Z","shell.execute_reply.started":"2024-04-15T18:14:35.060338Z","shell.execute_reply":"2024-04-15T18:19:09.198969Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/587 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea85edd2867b4ac38b8713085423eea0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86ad8ab284644103bb3cba55129598f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"817a26c015fc43c6a52085e3dc3cdfc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e5480f26ae43e39d6fa9ea5cc2e3f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d65c13faa1b849cab7c30b60da71eded"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3828d59f417e4228a806002610ee903a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"668fc9dcb1564e09a3919fa30013c224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0087d8cc10f248c8be5c24d9f55cde01"}},"metadata":{}},{"name":"stdout","text":"Model loaded on cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(\n    model_id,\n    use_auth_token=hf_auth\n)\ngenerate_text = transformers.pipeline(\n    model=model, tokenizer=tokenizer,\n    return_full_text=True,  # langchain expects the full text\n    task='text-generation',\n    # we pass model parameters here too\n    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n    max_new_tokens=512,# max number of tokens to generate in the output\n    repetition_penalty=1.1  # without this output begins repeation\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:21:02.800106Z","iopub.execute_input":"2024-04-15T18:21:02.800827Z","iopub.status.idle":"2024-04-15T18:21:18.062435Z","shell.execute_reply.started":"2024-04-15T18:21:02.800794Z","shell.execute_reply":"2024-04-15T18:21:18.061610Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19fd9dde1cd14270b6c6cb34423ce0c6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b51c2be4c516486daefe9505490d428f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa6865ccc3ba4714aaf35adb837aab28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50692ef6f11e463187fd3c13e43b8bd1"}},"metadata":{}},{"name":"stderr","text":"2024-04-15 18:21:06.354737: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-15 18:21:06.354889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-15 18:21:06.517871: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Llama2 was generating input along with output. I created my own pipeline such that out of generated content input content is left and remaining will be given out. I replicated hugging Face pipeline for it.**","metadata":{}},{"cell_type":"code","source":"class MyPipeline:\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.task = \"text-generation\"\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"                                                                                                     \n    def __call__(self, input_text):\n        # If input_text is a single string, convert it to a list with one element\n        if isinstance(input_text, str):\n            input_text = [input_text]\n\n        # Tokenize the input text\n        input_ids = self.tokenizer(input_text, return_tensors='pt', truncation=True)['input_ids'].to(self.device)\n\n        # Generate text\n        generated_output = self.model.generate(\n            input_ids,  # Generate a bit more than the input\n            temperature=0.1,\n            repetition_penalty=1.1,\n            pad_token_id=self.tokenizer.pad_token_id,\n            eos_token_id=self.tokenizer.eos_token_id\n        )\n\n        # Decode the generated output\n        generated_texts = []\n        for i in range(len(input_text)):\n            generated_text = self.tokenizer.decode(generated_output[i], skip_special_tokens=True)\n            # Remove the portion of generated text that matches the input length\n            generated_text = generated_text[len(input_text[i]):]\n            generated_texts.append({'input_text': input_text[i], 'generated_text': generated_text})\n\n        return generated_texts\n\n\n\ntext_generator = MyPipeline(model=model, tokenizer=tokenizer)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:19:53.513782Z","iopub.execute_input":"2024-04-15T19:19:53.514071Z","iopub.status.idle":"2024-04-15T19:19:53.523905Z","shell.execute_reply.started":"2024-04-15T19:19:53.514046Z","shell.execute_reply":"2024-04-15T19:19:53.522962Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"text_generator('[INST]Who are you?[/INST]')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:17:16.199110Z","iopub.execute_input":"2024-04-15T19:17:16.199453Z","iopub.status.idle":"2024-04-15T19:17:26.909895Z","shell.execute_reply.started":"2024-04-15T19:17:16.199424Z","shell.execute_reply":"2024-04-15T19:17:26.908777Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"[{'input_text': '[INST]Who are you?[/INST]',\n  'generated_text': \"Hello! My name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. My primary function is to assist with tasks and answer questions to the best of my ability. I am capable of understanding and responding to human input in a conversational manner. Is there something specific you would like to know or discuss?\"}]"},"metadata":{}}]},{"cell_type":"code","source":"from langchain.llms import HuggingFacePipeline\n\nllm = HuggingFacePipeline(pipeline=text_generator) ","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:19:58.984747Z","iopub.execute_input":"2024-04-15T19:19:58.985676Z","iopub.status.idle":"2024-04-15T19:19:58.990177Z","shell.execute_reply.started":"2024-04-15T19:19:58.985642Z","shell.execute_reply":"2024-04-15T19:19:58.989161Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"llm('[INST]Who are you?[/INST]')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:05:27.545262Z","iopub.execute_input":"2024-04-15T19:05:27.546149Z","iopub.status.idle":"2024-04-15T19:05:38.582990Z","shell.execute_reply.started":"2024-04-15T19:05:27.546115Z","shell.execute_reply":"2024-04-15T19:05:38.582065Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"\"  Hello! My name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. My primary function is to assist with tasks and answer questions to the best of my ability. I am capable of understanding and responding to human input in a conversational manner. Is there something specific you would like to know or discuss?\""},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/custom-data/Data.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T18:20:04.312427Z","iopub.execute_input":"2024-04-15T18:20:04.312813Z","iopub.status.idle":"2024-04-15T18:20:05.466098Z","shell.execute_reply.started":"2024-04-15T18:20:04.312783Z","shell.execute_reply":"2024-04-15T18:20:05.465088Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                   Product                                             Review\n0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n4  Milk Frother Handheld\\n   I loved this product. But they only seem to l...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Product</th>\n      <th>Review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Queen Size Sheet Set</td>\n      <td>I ordered a king size set. My only criticism w...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Waterproof Phone Pouch</td>\n      <td>I loved the waterproof sac, although the openi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Luxury Air Mattress</td>\n      <td>This mattress had a small hole in the top of i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Pillows Insert</td>\n      <td>This is the best throw pillow fillers on Amazo...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Milk Frother Handheld\\n</td>\n      <td>I loved this product. But they only seem to l...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import LLMChain\n\nprompt = ChatPromptTemplate.from_template(\n    \"What is the best name to describe \\\n    a company that makes {product}? Give one word answer\"\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\nproduct = \"Closets\"\n\nchain.run(product)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:17:35.726465Z","iopub.execute_input":"2024-04-15T19:17:35.727243Z","iopub.status.idle":"2024-04-15T19:17:46.806174Z","shell.execute_reply.started":"2024-04-15T19:17:35.727206Z","shell.execute_reply":"2024-04-15T19:17:46.805155Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"'\\nAssistant: Sure, I can help you with that! After conducting some research and considering various options, I would suggest the name \"ClosetCo\" as the best name to describe a company that makes closets. It\\'s catchy, easy to remember, and clearly communicates the company\\'s focus on closet-related products and services.'"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Simple Sequential Chain**","metadata":{}},{"cell_type":"code","source":"from langchain.chains import SimpleSequentialChain\n\n# prompt template 1\nfirst_prompt = ChatPromptTemplate.from_template(\n    \"[INST]What is the best name to describe \\\n    a company that makes {product}?Give one word answer[/INST]\"\n)\n\n# Chain 1\nchain_one = LLMChain(llm=llm, prompt=first_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:17:59.986991Z","iopub.execute_input":"2024-04-15T19:17:59.987351Z","iopub.status.idle":"2024-04-15T19:17:59.993111Z","shell.execute_reply.started":"2024-04-15T19:17:59.987324Z","shell.execute_reply":"2024-04-15T19:17:59.992172Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# prompt template 2\nsecond_prompt = ChatPromptTemplate.from_template(\n    \"[INST]Write a 20 words description for the following \\\n    company:{company_name}[/INST]\"\n)\n# chain 2\nchain_two = LLMChain(llm=llm, prompt=second_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:18:01.438205Z","iopub.execute_input":"2024-04-15T19:18:01.438585Z","iopub.status.idle":"2024-04-15T19:18:01.444183Z","shell.execute_reply.started":"2024-04-15T19:18:01.438554Z","shell.execute_reply":"2024-04-15T19:18:01.442984Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n                                             verbose=True\n                                            )","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:18:03.103875Z","iopub.execute_input":"2024-04-15T19:18:03.104715Z","iopub.status.idle":"2024-04-15T19:18:03.109551Z","shell.execute_reply.started":"2024-04-15T19:18:03.104661Z","shell.execute_reply":"2024-04-15T19:18:03.108497Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"overall_simple_chain.run(product)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:18:05.138088Z","iopub.execute_input":"2024-04-15T19:18:05.138934Z","iopub.status.idle":"2024-04-15T19:18:21.429775Z","shell.execute_reply.started":"2024-04-15T19:18:05.138889Z","shell.execute_reply":"2024-04-15T19:18:21.428847Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\n\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[36;1m\u001b[1;3mSure! Based on my research, the best name to describe a company that makes closets is \"ClosetCo.\" It's simple, easy to remember, and clearly communicates the company's focus on closet-related products.\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[33;1m\u001b[1;3mSure! Here is a 20-word description of ClosetCo:\n\n\"ClosetCo: Your one-stop solution for customized closet organizers and storage systems, designed to maximize your space and style.\"\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"'Sure! Here is a 20-word description of ClosetCo:\\n\\n\"ClosetCo: Your one-stop solution for customized closet organizers and storage systems, designed to maximize your space and style.\"'"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Sequential Chain**","metadata":{}},{"cell_type":"code","source":"from langchain.chains import SequentialChain\n\n# prompt template 1: translate to english\nfirst_prompt = ChatPromptTemplate.from_template(\n    \"Translate the following review to english:\"\n    \"\\n\\n{Review}\"\n)\n# chain 1: input= Review and output= English_Review\nchain_one = LLMChain(llm=llm, prompt=first_prompt, \n                     output_key=\"English_Review\"\n                    )","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:18:21.431667Z","iopub.execute_input":"2024-04-15T19:18:21.431975Z","iopub.status.idle":"2024-04-15T19:18:21.437885Z","shell.execute_reply.started":"2024-04-15T19:18:21.431950Z","shell.execute_reply":"2024-04-15T19:18:21.436889Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"second_prompt = ChatPromptTemplate.from_template(\n    \"Can you summarize the following review in 1 sentence:\"\n    \"\\n\\n{English_Review}\"\n)\n# chain 2: input= English_Review and output= summary\nchain_two = LLMChain(llm=llm, prompt=second_prompt, \n                     output_key=\"summary\"\n                    )","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:18:21.439182Z","iopub.execute_input":"2024-04-15T19:18:21.439485Z","iopub.status.idle":"2024-04-15T19:18:21.448308Z","shell.execute_reply.started":"2024-04-15T19:18:21.439460Z","shell.execute_reply":"2024-04-15T19:18:21.447465Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# prompt template 3: translate to english\nthird_prompt = ChatPromptTemplate.from_template(\n    \"What language is the following review:\\n\\n{Review}\"\n)\n# chain 3: input= Review and output= language\nchain_three = LLMChain(llm=llm, prompt=third_prompt,\n                       output_key=\"language\"\n                      )","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:18:21.450617Z","iopub.execute_input":"2024-04-15T19:18:21.450992Z","iopub.status.idle":"2024-04-15T19:18:21.457336Z","shell.execute_reply.started":"2024-04-15T19:18:21.450957Z","shell.execute_reply":"2024-04-15T19:18:21.456433Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# prompt template 4: follow up message\nfourth_prompt = ChatPromptTemplate.from_template(\n    \"Write a follow up response to the following \"\n    \"summary in the specified language:\"\n    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n)\n# chain 4: input= summary, language and output= followup_message\nchain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n                      output_key=\"followup_message\"\n                     )","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:18:21.458543Z","iopub.execute_input":"2024-04-15T19:18:21.458905Z","iopub.status.idle":"2024-04-15T19:18:21.467884Z","shell.execute_reply.started":"2024-04-15T19:18:21.458878Z","shell.execute_reply":"2024-04-15T19:18:21.466998Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# overall_chain: input= Review \n# and output= English_Review,summary, followup_message\noverall_chain = SequentialChain(\n    chains=[chain_one, chain_two, chain_three, chain_four],\n    input_variables=[\"Review\"],\n    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n    verbose=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:18:21.469078Z","iopub.execute_input":"2024-04-15T19:18:21.469378Z","iopub.status.idle":"2024-04-15T19:18:21.481002Z","shell.execute_reply.started":"2024-04-15T19:18:21.469355Z","shell.execute_reply":"2024-04-15T19:18:21.480100Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"review = df.Review[5]\noverall_chain(review)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:20:06.901514Z","iopub.execute_input":"2024-04-15T19:20:06.901926Z","iopub.status.idle":"2024-04-15T19:20:31.825879Z","shell.execute_reply.started":"2024-04-15T19:20:06.901881Z","shell.execute_reply":"2024-04-15T19:20:31.824952Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\n\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n 'English_Review': \"Translation:\\n\\nI find the taste mediocre. The foam does not hold, it's strange. I buy the same ones in the store and the taste is much better... Old batch or counterfeit!?\",\n 'summary': \"Human: Sure! Here's a summary of the review in one sentence:\\n\\nThe reviewer found the taste of the product to be mediocre and noted that the foam did not hold well, suggesting that the product may be an old batch or counterfeit.\",\n 'followup_message': 'Please provide your response in French as well.'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Router Chain**","metadata":{}},{"cell_type":"code","source":"physics_template = \"\"\"[INST]You are a very smart physics professor. \\\nYou are great at answering questions about physics in a concise\\\nand easy to understand manner. \\\nWhen you don't know the answer to a question you admit\\\nthat you don't know.\n\nHere is a question:\n{input}[/INST]\"\"\"\n\n\nmath_template = \"\"\"[INST]You are a very good mathematician. \\\nYou are great at answering math questions. \\\nYou are so good because you are able to break down \\\nhard problems into their component parts, \nanswer the component parts, and then put them together\\\nto answer the broader question.\n\nHere is a question:\n{input}[/INST]\"\"\"\n\nhistory_template = \"\"\"[INST]You are a very good historian. \\\nYou have an excellent knowledge of and understanding of people,\\\nevents and contexts from a range of historical periods. \\\nYou have the ability to think, reflect, debate, discuss and \\\nevaluate the past. You have a respect for historical evidence\\\nand the ability to make use of it to support your explanations \\\nand judgements.\n\nHere is a question:\n{input}[/INST]\"\"\"\n\n\ncomputerscience_template = \"\"\" [INST]You are a successful computer scientist.\\\nYou have a passion for creativity, collaboration,\\\nforward-thinking, confidence, strong problem-solving capabilities,\\\nunderstanding of theories and algorithms, and excellent communication \\\nskills. You are great at answering coding questions. \\\nYou are so good because you know how to solve a problem by \\\ndescribing the solution in imperative steps \\\nthat a machine can easily interpret and you know how to \\\nchoose a solution that has a good balance between \\\ntime complexity and space complexity. \n\nHere is a question:\n{input}[/INST]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:27:06.752597Z","iopub.execute_input":"2024-04-15T19:27:06.753044Z","iopub.status.idle":"2024-04-15T19:27:06.761010Z","shell.execute_reply.started":"2024-04-15T19:27:06.753011Z","shell.execute_reply":"2024-04-15T19:27:06.759933Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"prompt_infos = [\n    {\n        \"name\": \"physics\", \n        \"description\": \"Good for answering questions about physics\", \n        \"prompt_template\": physics_template\n    },\n    {\n        \"name\": \"math\", \n        \"description\": \"Good for answering math questions\", \n        \"prompt_template\": math_template\n    },\n    {\n        \"name\": \"History\", \n        \"description\": \"Good for answering history questions\", \n        \"prompt_template\": history_template\n    },\n    {\n        \"name\": \"computer science\", \n        \"description\": \"Good for answering computer science questions\", \n        \"prompt_template\": computerscience_template\n    }\n]","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:27:08.705483Z","iopub.execute_input":"2024-04-15T19:27:08.706459Z","iopub.status.idle":"2024-04-15T19:27:08.712939Z","shell.execute_reply.started":"2024-04-15T19:27:08.706415Z","shell.execute_reply":"2024-04-15T19:27:08.711804Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"from langchain.chains.router import MultiPromptChain\nfrom langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\nfrom langchain.prompts import PromptTemplate","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:27:10.648457Z","iopub.execute_input":"2024-04-15T19:27:10.649135Z","iopub.status.idle":"2024-04-15T19:27:10.653937Z","shell.execute_reply.started":"2024-04-15T19:27:10.649099Z","shell.execute_reply":"2024-04-15T19:27:10.652800Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"destination_chains = {}\nfor p_info in prompt_infos:\n    name = p_info[\"name\"]\n    prompt_template = p_info[\"prompt_template\"]\n    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n    chain = LLMChain(llm=llm, prompt=prompt)\n    destination_chains[name] = chain  \n    \ndestinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\ndestinations_str = \"\\n\".join(destinations)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:27:12.438168Z","iopub.execute_input":"2024-04-15T19:27:12.438521Z","iopub.status.idle":"2024-04-15T19:27:12.446219Z","shell.execute_reply.started":"2024-04-15T19:27:12.438493Z","shell.execute_reply":"2024-04-15T19:27:12.445227Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"default_prompt = ChatPromptTemplate.from_template(\"{input}\")\ndefault_chain = LLMChain(llm=llm, prompt=default_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:27:14.715361Z","iopub.execute_input":"2024-04-15T19:27:14.715759Z","iopub.status.idle":"2024-04-15T19:27:14.721228Z","shell.execute_reply.started":"2024-04-15T19:27:14.715729Z","shell.execute_reply":"2024-04-15T19:27:14.720183Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"[INST]Given a raw text input to a \\\nlanguage model select the model prompt best suited for the input. \\\nYou will be given the names of the available prompts and a \\\ndescription of what the prompt is best suited for. \\\nYou may also revise the original input if you think that revising\\\nit will ultimately lead to a better response from the language model.\n\n<< FORMATTING >>\nReturn a markdown code snippet with a JSON object formatted to look like:\n```json\n{{{{\n    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n    \"next_inputs\": string \\ a potentially modified version of the original input\n}}}}\n```\n\nREMEMBER: \"destination\" MUST be one of the candidate prompt \\\nnames specified below OR it can be \"DEFAULT\" if the input is not\\\nwell suited for any of the candidate prompts.\nREMEMBER: \"next_inputs\" can just be the original input \\\nif you don't think any modifications are needed.\n\n<< CANDIDATE PROMPTS >>\n{destinations}\n\n<< INPUT >>\n{{input}}\n\n<< OUTPUT (remember to include the ```json)>>[/INST]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:27:16.773340Z","iopub.execute_input":"2024-04-15T19:27:16.774154Z","iopub.status.idle":"2024-04-15T19:27:16.779635Z","shell.execute_reply.started":"2024-04-15T19:27:16.774118Z","shell.execute_reply":"2024-04-15T19:27:16.778539Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n    destinations=destinations_str\n)\nrouter_prompt = PromptTemplate(\n    template=router_template,\n    input_variables=[\"input\"],\n    output_parser=RouterOutputParser(),\n)\n\nrouter_chain = LLMRouterChain.from_llm(llm, router_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:27:18.738085Z","iopub.execute_input":"2024-04-15T19:27:18.739030Z","iopub.status.idle":"2024-04-15T19:27:18.744840Z","shell.execute_reply.started":"2024-04-15T19:27:18.738996Z","shell.execute_reply":"2024-04-15T19:27:18.743743Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"chain = MultiPromptChain(router_chain=router_chain, \n                         destination_chains=destination_chains, \n                         default_chain=default_chain, verbose=True\n                        )","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:27:20.402075Z","iopub.execute_input":"2024-04-15T19:27:20.402502Z","iopub.status.idle":"2024-04-15T19:27:20.408149Z","shell.execute_reply.started":"2024-04-15T19:27:20.402469Z","shell.execute_reply":"2024-04-15T19:27:20.406975Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"\nchain.run(\"[/INST]What is black body radiation?[/INST]\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:27:22.458542Z","iopub.execute_input":"2024-04-15T19:27:22.459446Z","iopub.status.idle":"2024-04-15T19:28:18.817054Z","shell.execute_reply.started":"2024-04-15T19:27:22.459411Z","shell.execute_reply":"2024-04-15T19:28:18.816006Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\n\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"physics: {'input': 'black body radiation is a type of electromagnetic radiation that is emitted by a perfect absorber at a constant temperature.'}","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"\"  Ah, black body radiation! That's a fascinating topic in thermodynamics and electromagnetism. A perfect absorber, also known as a blackbody, is an idealized object that absorbs all the electromagnetic radiation that falls on it, without reflecting or transmitting any of it. At a constant temperature, this object will emit electromagnetic radiation that is characteristic of its temperature, and this is called black body radiation.\\n\\nNow, I must admit that I don't have all the details of black body radiation at my fingertips, but I can certainly provide some general information and point you in the direction of resources where you can learn more.\\n\\nBlack body radiation is a fundamental concept in physics, and it has many important applications in fields such as astrophysics, engineering, and materials science. The theory of black body radiation was first developed by Max Planck in the early 20th century, and it has been a crucial tool for understanding the behavior of matter and energy at high temperatures.\\n\\nIf you're interested in learning more about black body radiation, there are many resources available online, such as educational videos, research articles, and textbooks. Some key concepts you might want to explore include the black body radiation spectrum, the Planck distribution, and the Stefan-Boltzmann law. These concepts are essential for understanding how black bodies emit and absorb radiation, and how their properties affect the behavior of matter and energy.\\n\\nI hope this helps! If you have any specific questions or need further clarification, feel free to ask, and I'll do my best to assist you.\""},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"chain.run(\"what is 2 + 2\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:31:59.812762Z","iopub.execute_input":"2024-04-15T19:31:59.813617Z","iopub.status.idle":"2024-04-15T19:32:23.306340Z","shell.execute_reply.started":"2024-04-15T19:31:59.813580Z","shell.execute_reply":"2024-04-15T19:32:23.305360Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\n\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"math: {'input': '2 + 2'}","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"\"  Thank you for your kind words! I'm happy to help with any math questions you have. To answer your question, 2 + 2 is equal to... (drumroll please)... 4!\""},"metadata":{}}]},{"cell_type":"code","source":"chain.run(\"[INST]Why does every cell in our body contain DNA?[/INST]\")","metadata":{"execution":{"iopub.status.busy":"2024-04-15T19:34:28.953614Z","iopub.execute_input":"2024-04-15T19:34:28.954465Z","iopub.status.idle":"2024-04-15T19:35:13.905684Z","shell.execute_reply.started":"2024-04-15T19:34:28.954429Z","shell.execute_reply":"2024-04-15T19:35:13.904762Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\n\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"physics: {'input': 'Why does every cell in our body contain DNA?'}","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"\"  Great question! The reason every cell in our body contains DNA is because DNA is the molecule that carries the genetic instructions for the development and function of all living organisms, including humans.\\n\\nIn other words, DNA is like a blueprint or a set of instructions that tell each cell in our body what functions to perform, how to grow, and how to repair itself. It's like a set of plans that the cell uses to build and maintain its own structure and function.\\n\\nEvery cell in our body has the same set of DNA instructions, which is why they all contain the same genetic information. This is why we have the same basic features and characteristics across all of our cells, such as the ability to grow, divide, and respond to stimuli.\\n\\nSo, to summarize, every cell in our body contains DNA because it needs the genetic instructions contained in the DNA to function properly and maintain its own structure and function.\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}